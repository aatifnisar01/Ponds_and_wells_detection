{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b0a294",
   "metadata": {
    "id": "44b0a294"
   },
   "source": [
    "# Necessary Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mk8wo4294sqE",
   "metadata": {
    "id": "Mk8wo4294sqE"
   },
   "outputs": [],
   "source": [
    "#!pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lw8ZKR4_4sic",
   "metadata": {
    "id": "lw8ZKR4_4sic"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "from random import choice\n",
    "import re\n",
    "import csv\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Polygon, box\n",
    "from PIL import Image\n",
    "import requests\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "from skimage.util import img_as_ubyte\n",
    "from shapely.ops import unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "uhelOwSUaNUO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhelOwSUaNUO",
    "outputId": "fb6f7fec-3a36-453f-92bf-dad240c08f9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/core-stack/Documents/Ponds\n"
     ]
    }
   ],
   "source": [
    "cd /home/core-stack/Documents/Ponds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "V4A5YTpVaVsM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "V4A5YTpVaVsM",
    "outputId": "5d043893-251e-433d-8d09-fd0be4c7407b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/core-stack/Documents/Ponds'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd = os.getcwd()\n",
    "pwd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50490d61-ef3b-4aa6-8f31-7f888136c0a4",
   "metadata": {
    "id": "50490d61-ef3b-4aa6-8f31-7f888136c0a4"
   },
   "source": [
    "# Predefined paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18fe86a5-16ad-4f74-8fff-9fd7271389aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "18fe86a5-16ad-4f74-8fff-9fd7271389aa",
    "outputId": "6e5b39f4-1daa-4ff2-e466-3c947ecf26c9"
   },
   "outputs": [],
   "source": [
    "# Load GoeJSON of the block on which you want to compute the ponds\n",
    "gdf = gpd.read_file(\"Shapefiles/Masalia_mws.geojson\")\n",
    "\n",
    "# Zoom level\n",
    "zoom_level = 17\n",
    "\n",
    "# Folder paths where you want to save image tiles\n",
    "image_dir = \"Data/Zoom17/Masalia\"\n",
    "\n",
    "# Scale of image tile\n",
    "scale = 1           # scale of 1 = 256*256 dimensional image\n",
    "\n",
    "# load trained model\n",
    "model_path = \"Models/Ponds_best.pt\"\n",
    "\n",
    "# CSV file name where masks of detected object will be saved\n",
    "csv_file = \"CSV_Output/Masalia_Ponds.csv\"\n",
    "\n",
    "# Entropy threshold needed to calculate entropy (only in wet ponds case)\n",
    "entropy_threshold = 2.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RWtC0gO4wEP2",
   "metadata": {
    "id": "RWtC0gO4wEP2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Download Data for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MxCLV_TxuXe6",
   "metadata": {
    "id": "MxCLV_TxuXe6"
   },
   "source": [
    "Get Bounding boxes automatically from GeoJSON instead of manually drawing on GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MkelP7MRyF24",
   "metadata": {
    "id": "MkelP7MRyF24"
   },
   "outputs": [],
   "source": [
    "# Get the bounding box coordinates\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "\n",
    "# Define bounding box points\n",
    "topLeft = [minx, maxy]\n",
    "topRight = [maxx, maxy]\n",
    "bottomRight = [maxx, miny]\n",
    "bottomLeft = [minx, miny]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pNtTiywlwVYC",
   "metadata": {
    "id": "pNtTiywlwVYC"
   },
   "outputs": [],
   "source": [
    "base_url = \"https://mt1.google.com/vt/lyrs=s\"\n",
    "\n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir)\n",
    "    print(f\"Created the folder: {image_dir}\")\n",
    "else:\n",
    "    print(f\"The folder already exists: {image_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tAzJ_LGSwVVi",
   "metadata": {
    "id": "tAzJ_LGSwVVi"
   },
   "outputs": [],
   "source": [
    "from tile_downloader import download_map_tiles\n",
    "# Call the function to download map tiles\n",
    "download_map_tiles(base_url, image_dir, zoom_level, scale, topLeft, topRight, bottomLeft, bottomRight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6XJeWZ1Mm7r4",
   "metadata": {
    "id": "6XJeWZ1Mm7r4",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SAVE PREDICTIONS IN CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346697d-0761-43e3-8320-57571f0da27e",
   "metadata": {
    "id": "5346697d-0761-43e3-8320-57571f0da27e"
   },
   "outputs": [],
   "source": [
    "conf_thresholds = {\n",
    "    'Dry': 0.75,\n",
    "    'Wet': 0.6\n",
    "}\n",
    "\n",
    "class_names = ['Dry', 'Wet']\n",
    "\n",
    "\n",
    "class_abbreviations = {'Dry': 'D', 'Wet': 'W'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0-jdfCwYnUxQ",
   "metadata": {
    "id": "0-jdfCwYnUxQ"
   },
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CFxzVndBnrFh",
   "metadata": {
    "id": "CFxzVndBnrFh"
   },
   "outputs": [],
   "source": [
    "my_new_model = YOLO(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NRIKTsyUoLiE",
   "metadata": {
    "id": "NRIKTsyUoLiE"
   },
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zNs0fFuloLAC",
   "metadata": {
    "id": "zNs0fFuloLAC"
   },
   "outputs": [],
   "source": [
    "image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4rpgqgBvaMW",
   "metadata": {
    "id": "p4rpgqgBvaMW"
   },
   "outputs": [],
   "source": [
    "len(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aAsnzVwsQPJP",
   "metadata": {
    "id": "aAsnzVwsQPJP"
   },
   "outputs": [],
   "source": [
    "from image_processor import process_image, extract_xtile_ytile, tile_corners_to_latlon, calculate_tile_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "txqhubSPF73e",
   "metadata": {
    "id": "txqhubSPF73e"
   },
   "outputs": [],
   "source": [
    "max_vertices = 0\n",
    "image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "# To store processed data from process_image\n",
    "image_data = []\n",
    "\n",
    "# First Pass: Find max vertices in polygons and store image data\n",
    "for image_path in image_files:\n",
    "    _, _, polygons, pred_classes, conf_scores, entropies = process_image(\n",
    "        image_path, conf_thresholds, my_new_model, class_names)  # Add the missing arguments\n",
    "    if polygons:\n",
    "        max_vertices = max(max_vertices, max(len(polygon) for polygon in polygons))\n",
    "    image_data.append({\n",
    "        'image_path': image_path,\n",
    "        'polygons': polygons,\n",
    "        'pred_classes': pred_classes,\n",
    "        'entropies': entropies\n",
    "    })\n",
    "\n",
    "# Create dynamic column headers for X/Y coordinates\n",
    "coordinate_headers = []\n",
    "for i in range(1, max_vertices + 1):\n",
    "    coordinate_headers.append(f\"X_{i}\")\n",
    "    coordinate_headers.append(f\"Y_{i}\")\n",
    "\n",
    "# Full header row\n",
    "header = [\"Image Path\", \"Predicted Class\", \"Center Latitude\", \"Center Longitude\",\n",
    "          \"Top Left Latitude\", \"Top Left Longitude\", \"Top Right Latitude\", \"Top Right Longitude\",\n",
    "          \"Bottom Left Latitude\", \"Bottom Left Longitude\", \"Bottom Right Latitude\", \"Bottom Right Longitude\"] + coordinate_headers\n",
    "\n",
    "# Process and save to CSV\n",
    "start_time = time.time()\n",
    "\n",
    "with open(csv_file, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(header)  # Write dynamic header row\n",
    "\n",
    "    for data in image_data:\n",
    "        image_path = data['image_path']\n",
    "        polygons = data['polygons']\n",
    "        pred_classes = data['pred_classes']\n",
    "        entropies = data['entropies']\n",
    "\n",
    "        if image_path is None:\n",
    "            continue\n",
    "\n",
    "        xtile, ytile = extract_xtile_ytile(image_path)\n",
    "        top_left, top_right, bottom_left, bottom_right = tile_corners_to_latlon(xtile, ytile, zoom)\n",
    "        latitude, longitude = calculate_tile_center(top_left, top_right, bottom_left, bottom_right)\n",
    "\n",
    "        for pred_class, polygon, entropy_value in zip(pred_classes, polygons, entropies):\n",
    "            if entropy_value is None or entropy_value > entropy_threshold:\n",
    "                print(f\"Skipped {pred_class} due to high entropy: {entropy_value if entropy_value is not None else 'None'}\")\n",
    "                continue\n",
    "\n",
    "            row = [image_path, pred_class, latitude, longitude, top_left[0], top_left[1],\n",
    "                   top_right[0], top_right[1], bottom_left[0], bottom_left[1], bottom_right[0], bottom_right[1]]\n",
    "\n",
    "            # Flatten polygon coordinates while ensuring it matches max_vertices\n",
    "            flat_polygon = [coord for point in polygon for coord in point]\n",
    "            flat_polygon += [None] * (2 * max_vertices - len(flat_polygon))  # Fill missing values with None\n",
    "\n",
    "            row.extend(flat_polygon)\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"CSV file '{csv_file}' saved successfully.\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hZptNR-tTe9R",
   "metadata": {
    "id": "hZptNR-tTe9R"
   },
   "source": [
    "# Add Buffer to combine nearby predicted objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f70ee5-70ec-487f-8ba1-1a72010eb732",
   "metadata": {
    "id": "23f70ee5-70ec-487f-8ba1-1a72010eb732"
   },
   "outputs": [],
   "source": [
    "EARTH_CIRCUMFERENCE_DEGREES = 360  # degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e76b28-709b-4ff4-91d4-46f897c48543",
   "metadata": {
    "id": "68e76b28-709b-4ff4-91d4-46f897c48543"
   },
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "df.rename(columns={'Predicted Class': 'Class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9356336-872a-4944-9b62-ec653b86eaf1",
   "metadata": {
    "id": "a9356336-872a-4944-9b62-ec653b86eaf1"
   },
   "outputs": [],
   "source": [
    "# Extract the base name dynamically (e.g., \"TRY\" from \"TRY.csv\")\n",
    "csv_basename = os.path.splitext(os.path.basename(csv_file))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "-5H8fQokvr-W",
   "metadata": {
    "id": "-5H8fQokvr-W",
    "outputId": "8273653c-e5b5-45b0-e3d2-282e36f3f428"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_422764/123004970.py:63: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf_final['Buffered'] = gdf_final.geometry.buffer(buffer_distance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ZIP file: Shapefile_Output/Masalia_Ponds_COMBINED_GEOMETRY.zip\n",
      "Deleted: Shapefile_Output/Masalia_Ponds_COMBINED_GEOMETRY.dbf\n",
      "Deleted: Shapefile_Output/Masalia_Ponds_COMBINED_GEOMETRY.shx\n",
      "Deleted: Shapefile_Output/Masalia_Ponds_COMBINED_GEOMETRY.shp\n",
      "Deleted: Shapefile_Output/Masalia_Ponds_COMBINED_GEOMETRY.cpg\n",
      "Deleted: Shapefile_Output/Masalia_Ponds_COMBINED_GEOMETRY.prj\n",
      "All shapefile components have been deleted after zipping, except for the .csv file.\n"
     ]
    }
   ],
   "source": [
    "from geo_utils import pixel_to_geo\n",
    "\n",
    "# Initialize an empty list for GeoJSON features\n",
    "geojson_features = []\n",
    "\n",
    "# Iterate through each row\n",
    "for _, row in df.iterrows():\n",
    "    lat_top_left = row['Top Left Latitude']\n",
    "    lon_top_left = row['Top Left Longitude']\n",
    "    lat_bottom_right = row['Bottom Right Latitude']\n",
    "    lon_bottom_right = row['Bottom Right Longitude']\n",
    "\n",
    "    tile_width, tile_height = 256, 256\n",
    "\n",
    "    object_coords = []\n",
    "\n",
    "    # Iterate over all possible coordinate columns dynamically\n",
    "    i = 1\n",
    "    while True:\n",
    "        x_col = f'X_{i}'\n",
    "        y_col = f'Y_{i}'\n",
    "\n",
    "        if x_col not in row or y_col not in row:\n",
    "            break  # Stop if columns don't exist\n",
    "\n",
    "        x = row[x_col]\n",
    "        y = row[y_col]\n",
    "\n",
    "        if pd.notna(x) and pd.notna(y):\n",
    "            lon, lat = pixel_to_geo(x, y, lat_top_left, lon_top_left, lat_bottom_right, lon_bottom_right, tile_width, tile_height)\n",
    "            if np.isfinite(lon) and np.isfinite(lat):\n",
    "                object_coords.append((lon, lat))\n",
    "        else:\n",
    "            break  # Stop when NaN values appear\n",
    "\n",
    "        i += 1  # Move to the next set of coordinates\n",
    "\n",
    "    # Ensure the polygon has at least 3 points before adding\n",
    "    if len(object_coords) >= 3:\n",
    "        object_coords.append(object_coords[0])  # Close the polygon\n",
    "        polygon_geometry = Polygon(object_coords)\n",
    "        feature = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": polygon_geometry,\n",
    "            \"properties\": {\n",
    "                \"Class\": row['Class']\n",
    "            }\n",
    "        }\n",
    "        geojson_features.append(feature)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf_final = gpd.GeoDataFrame(\n",
    "    [feature['properties'] for feature in geojson_features],\n",
    "    geometry=[feature['geometry'] for feature in geojson_features],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Merge overlapping geometries\n",
    "\n",
    "\n",
    "# Merge overlapping geometries\n",
    "buffer_distance = 0.0005\n",
    "gdf_final['Buffered'] = gdf_final.geometry.buffer(buffer_distance)\n",
    "combined_polygons = unary_union(gdf_final['Buffered'])\n",
    "combined_polygons = combined_polygons.buffer(-buffer_distance)\n",
    "\n",
    "# Convert back to GeoDataFrame\n",
    "gdf_combined = gpd.GeoDataFrame(geometry=[combined_polygons], crs=\"EPSG:4326\")\n",
    "\n",
    "# Ensure the output folder exists\n",
    "output_folder = 'Shapefile_Output'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the final shapefile in the 'Shapefile_Output' folder\n",
    "shapefile_path = os.path.join(output_folder, f\"{csv_basename}_COMBINED_GEOMETRY.shp\")\n",
    "gdf_combined.to_file(shapefile_path)\n",
    "\n",
    "# Create the ZIP file in the 'Shapefile_Output' folder\n",
    "zip_filename = os.path.join(output_folder, f\"{csv_basename}_COMBINED_GEOMETRY.zip\")\n",
    "shapefile_files = glob.glob(os.path.join(output_folder, f\"{csv_basename}_COMBINED_GEOMETRY.*\"))\n",
    "\n",
    "# Exclude the .csv file from the list of files to be zipped\n",
    "shapefile_files = [file for file in shapefile_files if not file.endswith('.csv')]\n",
    "\n",
    "# Add the shapefile files to the zip archive\n",
    "with zipfile.ZipFile(zip_filename, \"w\") as zipf:\n",
    "    for file in shapefile_files:\n",
    "        zipf.write(file, os.path.basename(file))\n",
    "\n",
    "print(f\"Created ZIP file: {zip_filename}\")\n",
    "\n",
    "# Delete the original shapefile files after zipping, but keep the .csv file\n",
    "for file in shapefile_files:\n",
    "    os.remove(file)\n",
    "    print(f\"Deleted: {file}\")\n",
    "\n",
    "print(\"All shapefile components have been deleted after zipping, except for the .csv file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83e361-e515-4ed3-b37f-80375464a620",
   "metadata": {
    "id": "7b83e361-e515-4ed3-b37f-80375464a620"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "44b0a294",
    "RWtC0gO4wEP2",
    "6XJeWZ1Mm7r4",
    "hZptNR-tTe9R"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
