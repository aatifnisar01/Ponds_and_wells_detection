{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b0a294",
   "metadata": {
    "id": "44b0a294"
   },
   "source": [
    "# Necessary Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mk8wo4294sqE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88181,
     "status": "ok",
     "timestamp": 1740564548654,
     "user": {
      "displayName": "ICTD IITD",
      "userId": "11419946949516230456"
     },
     "user_tz": -330
    },
    "id": "Mk8wo4294sqE",
    "outputId": "53836785-398b-4bbd-9004-75974400d8a5"
   },
   "outputs": [],
   "source": [
    "#!pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lw8ZKR4_4sic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11696,
     "status": "ok",
     "timestamp": 1740564560362,
     "user": {
      "displayName": "ICTD IITD",
      "userId": "11419946949516230456"
     },
     "user_tz": -330
    },
    "id": "lw8ZKR4_4sic",
    "outputId": "5ed16b7f-04d6-4008-c66a-5866d26eead7"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "from random import choice\n",
    "import re\n",
    "import csv\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Polygon, box\n",
    "from PIL import Image\n",
    "import requests\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "from skimage.util import img_as_ubyte\n",
    "from shapely.ops import unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "uhelOwSUaNUO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2568,
     "status": "ok",
     "timestamp": 1740564562935,
     "user": {
      "displayName": "ICTD IITD",
      "userId": "11419946949516230456"
     },
     "user_tz": -330
    },
    "id": "uhelOwSUaNUO",
    "outputId": "fb6f7fec-3a36-453f-92bf-dad240c08f9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/core-stack/Documents/Ponds\n"
     ]
    }
   ],
   "source": [
    "cd /home/core-stack/Documents/Ponds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "V4A5YTpVaVsM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1740564562962,
     "user": {
      "displayName": "ICTD IITD",
      "userId": "11419946949516230456"
     },
     "user_tz": -330
    },
    "id": "V4A5YTpVaVsM",
    "outputId": "5d043893-251e-433d-8d09-fd0be4c7407b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/core-stack/Documents/Ponds'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd = os.getcwd()\n",
    "pwd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50490d61-ef3b-4aa6-8f31-7f888136c0a4",
   "metadata": {},
   "source": [
    "Paths and Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fe86a5-16ad-4f74-8fff-9fd7271389aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GoeJSON of the block on which you want to compute the ponds\n",
    "gdf = gpd.read_file(\"Shapefiles/Masalia_mws.geojson\")\n",
    "\n",
    "# Zoom level \n",
    "zoom_level = 17 \n",
    "\n",
    "# Folder paths where you want to save image tiles\n",
    "image_dir = \"Data/Zoom17/Masalia\"\n",
    "\n",
    "# Scale of image tile\n",
    "scale = 1           # scale of 1 = 256*256 dimensional image\n",
    "\n",
    "# load trained model\n",
    "model_path = \"Ponds_best.pt\"\n",
    "\n",
    "# CSV file name where masks of detected object will be saved\n",
    "csv_file = \"CSV_Output/Masalia_Ponds.csv\"\n",
    "\n",
    "# Entropy threshold needed to calculate entropy (only in wet ponds case)\n",
    "entropy_threshold = 2.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RWtC0gO4wEP2",
   "metadata": {
    "id": "RWtC0gO4wEP2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Download Data for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MxCLV_TxuXe6",
   "metadata": {
    "id": "MxCLV_TxuXe6"
   },
   "source": [
    "Get Bounding boxes automatically from GeoJSON instead of manually drawing on GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MkelP7MRyF24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1740565407123,
     "user": {
      "displayName": "ICTD IITD",
      "userId": "11419946949516230456"
     },
     "user_tz": -330
    },
    "id": "MkelP7MRyF24",
    "outputId": "74a20b49-ce39-49f8-a229-047813946296"
   },
   "outputs": [],
   "source": [
    "# Get the bounding box coordinates\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "\n",
    "# Define bounding box points\n",
    "topLeft = [minx, maxy]\n",
    "topRight = [maxx, maxy]\n",
    "bottomRight = [maxx, miny]\n",
    "bottomLeft = [minx, miny]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6PUmTAcbwJIq",
   "metadata": {
    "id": "6PUmTAcbwJIq"
   },
   "outputs": [],
   "source": [
    "# Helper function to convert latitude and longitude to tile numbers\n",
    "def deg2num(lat_deg, lon_deg, zoom):\n",
    "    lat_rad = math.radians(lat_deg)\n",
    "    n = 2.0 ** zoom\n",
    "    xtile = int((lon_deg + 180.0) / 360.0 * n)\n",
    "    ytile = int((1.0 - math.log(math.tan(lat_rad) + (1 / math.cos(lat_rad))) / math.pi) / 2.0 * n)\n",
    "    return (xtile, ytile)\n",
    "\n",
    "# Function to download map tiles\n",
    "def download_map_tiles(base_url, image_dir, zoom_level, scale, topLeft, topRight, bottomLeft, bottomRight):\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "    # Convert lat/lon to tile numbers and get bounding box\n",
    "    topleft = deg2num(topLeft[1], topLeft[0], zoom_level)\n",
    "    topright = deg2num(topRight[1], topRight[0], zoom_level)\n",
    "    bottomleft = deg2num(bottomLeft[1], bottomLeft[0], zoom_level)\n",
    "    bottomright = deg2num(bottomRight[1], bottomRight[0], zoom_level)\n",
    "    \n",
    "    xmin = min(topleft[0], topright[0], bottomleft[0], bottomright[0])\n",
    "    xmax = max(topleft[0], topright[0], bottomleft[0], bottomright[0])\n",
    "    ymin = min(topleft[1], topright[1], bottomleft[1], bottomright[1])\n",
    "    ymax = max(topleft[1], topright[1], bottomleft[1], bottomright[1])\n",
    "\n",
    "    # Get start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over each tile within the specified range\n",
    "    for x in range(xmin, xmax + 1):\n",
    "        for y in range(ymin, ymax + 1):\n",
    "            # Construct the URL for the current tile with scale=3 for 640x640\n",
    "            tile_url = f\"{base_url}&x={x}&y={y}&z={zoom_level}&scale={scale}\"\n",
    "            print(tile_url)\n",
    "            try:\n",
    "                # Send HTTP GET request to fetch the tile\n",
    "                response = requests.get(tile_url)\n",
    "\n",
    "                # Check if request was successful (status code 200)\n",
    "                if response.status_code == 200:\n",
    "                    # Save the tile image to a file in the output folder\n",
    "                    filename = f\"tile_{zoom_level}_{x}_{y}.png\"\n",
    "                    filepath = os.path.join(image_dir, filename)\n",
    "                    with open(filepath, \"wb\") as f:\n",
    "                        f.write(response.content)\n",
    "                    print(f\"Downloaded: {filename}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download tile ({x}, {y}), HTTP status code: {response.status_code}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading tile ({x}, {y}): {e}\")\n",
    "\n",
    "    # Get end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Print the total execution time\n",
    "    print(f\"Total time taken: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pNtTiywlwVYC",
   "metadata": {
    "id": "pNtTiywlwVYC"
   },
   "outputs": [],
   "source": [
    "base_url = \"https://mt1.google.com/vt/lyrs=s\"\n",
    "\n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir)\n",
    "    print(f\"Created the folder: {image_dir}\")\n",
    "else:\n",
    "    print(f\"The folder already exists: {image_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tAzJ_LGSwVVi",
   "metadata": {
    "id": "tAzJ_LGSwVVi"
   },
   "outputs": [],
   "source": [
    "# Call the function to download map tiles\n",
    "download_map_tiles(base_url, image_dir, zoom_level, scale, topLeft, topRight, bottomLeft, bottomRight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6XJeWZ1Mm7r4",
   "metadata": {
    "id": "6XJeWZ1Mm7r4"
   },
   "source": [
    "# SAVE PREDICTIONS IN CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346697d-0761-43e3-8320-57571f0da27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_thresholds = {\n",
    "    'Dry': 0.75,\n",
    "    'Wet': 0.6\n",
    "}\n",
    "\n",
    "class_names = ['Dry', 'Wet']\n",
    "\n",
    "\n",
    "class_abbreviations = {'Dry': 'D', 'Wet': 'W'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0-jdfCwYnUxQ",
   "metadata": {
    "id": "0-jdfCwYnUxQ"
   },
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CFxzVndBnrFh",
   "metadata": {
    "id": "CFxzVndBnrFh"
   },
   "outputs": [],
   "source": [
    "my_new_model = YOLO(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mVrn0LkExd_l",
   "metadata": {
    "id": "mVrn0LkExd_l"
   },
   "source": [
    "Function to calculate entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qv-7ND3VVl0E",
   "metadata": {
    "id": "qv-7ND3VVl0E"
   },
   "outputs": [],
   "source": [
    "def get_entropy(img, mask):\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32)  # Convert to grayscale\n",
    "    mask[mask > 0] = 1  # Ensure mask is binary\n",
    "    if mask.shape[:2] != img_gray.shape:\n",
    "        print(\"Mask shape does not match image shape\")\n",
    "        return 0\n",
    "\n",
    "    # Normalize the grayscale image to [0, 1]\n",
    "    img_gray = (img_gray - img_gray.min()) / (img_gray.max() - img_gray.min())\n",
    "\n",
    "    # Convert to uint8 after normalization\n",
    "    img_gray = img_as_ubyte(img_gray)\n",
    "\n",
    "    ent = entropy(img_gray.copy(), disk(5), mask=mask)\n",
    "    ent = ent[ent > 5.2]\n",
    "    if np.sum(mask) > 0:\n",
    "        ent = np.sum(ent) / np.sum(mask)  # Average entropy based on the mask area\n",
    "    else:\n",
    "        ent = 0\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NOuYYLEtoIFU",
   "metadata": {
    "id": "NOuYYLEtoIFU"
   },
   "source": [
    "Functions to save segmented/detected object in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jS-Ny2QNnsfE",
   "metadata": {
    "id": "jS-Ny2QNnsfE"
   },
   "outputs": [],
   "source": [
    "def process_image(image_path, conf_thresholds):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Unable to load image {image_path}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    results = my_new_model.predict(img)\n",
    "\n",
    "    polygons = []\n",
    "    pred_classes = []\n",
    "    conf_scores = []\n",
    "    entropies = []\n",
    "\n",
    "    if results[0].masks is not None:\n",
    "        for i, (polygon, cls, conf) in enumerate(zip(results[0].masks.xy, results[0].boxes.cls.cpu().numpy(), results[0].boxes.conf.cpu().numpy())):\n",
    "            class_name = class_names[int(cls)]\n",
    "            if conf >= conf_thresholds[class_name]:\n",
    "                polygons.append(polygon)\n",
    "                pred_classes.append(class_name)\n",
    "                conf_scores.append(conf)\n",
    "\n",
    "                if class_name == 'Wet':  # Only compute entropy for Wet class\n",
    "                    predicted_mask = results[0].masks.data.cpu().numpy()[i].astype(np.uint8)  # Convert to uint8\n",
    "                    entropy_value = get_entropy(img, predicted_mask)  # Calculate entropy for the masked region\n",
    "                    entropies.append(entropy_value)\n",
    "                    print(f\"Entropy for {class_name}: {entropy_value:.2f}\")\n",
    "                else:\n",
    "                    entropies.append(None)  # Skip entropy calculation for Dry class\n",
    "\n",
    "    return image_path, len(polygons), polygons, pred_classes, conf_scores, entropies\n",
    "\n",
    "\n",
    "def extract_xtile_ytile(image_path):\n",
    "    try:\n",
    "        basename = os.path.basename(image_path)\n",
    "        parts = basename.split('_')\n",
    "        if len(parts) >= 4:\n",
    "            xtile = int(parts[2])\n",
    "            ytile = int(parts[3].split('.')[0].split()[0])\n",
    "            return xtile, ytile\n",
    "        else:\n",
    "            raise ValueError(\"Filename does not contain valid tile coordinates\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Filename {image_path} does not contain valid tile coordinates: {e}\")\n",
    "\n",
    "def tile_corners_to_latlon(xtile, ytile, zoom):\n",
    "    n = 2.0 ** zoom\n",
    "    lon_deg = xtile / n * 360.0 - 180.0\n",
    "    lat_rad_nw = math.atan(math.sinh(math.pi * (1 - 2 * (ytile / n))))\n",
    "    lat_deg_nw = math.degrees(lat_rad_nw)\n",
    "\n",
    "    lat_rad_se = math.atan(math.sinh(math.pi * (1 - 2 * ((ytile + 1) / n))))\n",
    "    lat_deg_se = math.degrees(lat_rad_se)\n",
    "\n",
    "    lat_deg_nw = max(min(lat_deg_nw, 85.0511), -85.0511)\n",
    "    lat_deg_se = max(min(lat_deg_se, 85.0511), -85.0511)\n",
    "\n",
    "    top_left = (lat_deg_nw, lon_deg)\n",
    "    top_right = (lat_deg_nw, lon_deg + (360.0 / n))\n",
    "    bottom_right = (lat_deg_se, lon_deg + (360.0 / n))\n",
    "    bottom_left = (lat_deg_se, lon_deg)\n",
    "\n",
    "    return top_left, top_right, bottom_left, bottom_right\n",
    "\n",
    "def calculate_tile_center(top_left, top_right, bottom_left, bottom_right):\n",
    "    center_lat = (top_left[0] + bottom_left[0]) / 2\n",
    "    center_lon = (top_left[1] + top_right[1]) / 2\n",
    "    return (center_lat, center_lon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NRIKTsyUoLiE",
   "metadata": {
    "id": "NRIKTsyUoLiE"
   },
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zNs0fFuloLAC",
   "metadata": {
    "id": "zNs0fFuloLAC"
   },
   "outputs": [],
   "source": [
    "image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4rpgqgBvaMW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740465704502,
     "user": {
      "displayName": "ICTD IITD",
      "userId": "11419946949516230456"
     },
     "user_tz": -330
    },
    "id": "p4rpgqgBvaMW",
    "outputId": "43452bb5-9363-4998-9bf0-85286d8a9546"
   },
   "outputs": [],
   "source": [
    "len(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "txqhubSPF73e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3386,
     "status": "ok",
     "timestamp": 1740465807764,
     "user": {
      "displayName": "ICTD IITD",
      "userId": "11419946949516230456"
     },
     "user_tz": -330
    },
    "id": "txqhubSPF73e",
    "outputId": "49e94018-2a0f-4a15-a808-78c433291c65"
   },
   "outputs": [],
   "source": [
    "max_vertices = 0\n",
    "image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "# To store processed data from process_image\n",
    "image_data = []\n",
    "\n",
    "# First Pass: Find max vertices in polygons and store image data\n",
    "for image_path in image_files:\n",
    "    _, _, polygons, pred_classes, conf_scores, entropies = process_image(image_path, conf_thresholds)\n",
    "    if polygons:\n",
    "        max_vertices = max(max_vertices, max(len(polygon) for polygon in polygons))\n",
    "    image_data.append({\n",
    "        'image_path': image_path,\n",
    "        'polygons': polygons,\n",
    "        'pred_classes': pred_classes,\n",
    "        'entropies': entropies\n",
    "    })\n",
    "\n",
    "# Create dynamic column headers for X/Y coordinates\n",
    "coordinate_headers = []\n",
    "for i in range(1, max_vertices + 1):\n",
    "    coordinate_headers.append(f\"X_{i}\")\n",
    "    coordinate_headers.append(f\"Y_{i}\")\n",
    "\n",
    "# Full header row\n",
    "header = [\"Image Path\", \"Predicted Class\", \"Center Latitude\", \"Center Longitude\",\n",
    "          \"Top Left Latitude\", \"Top Left Longitude\", \"Top Right Latitude\", \"Top Right Longitude\",\n",
    "          \"Bottom Left Latitude\", \"Bottom Left Longitude\", \"Bottom Right Latitude\", \"Bottom Right Longitude\"] + coordinate_headers\n",
    "\n",
    "# Process and save to CSV\n",
    "start_time = time.time()\n",
    "\n",
    "with open(csv_file, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(header)  # Write dynamic header row\n",
    "\n",
    "    for data in image_data:\n",
    "        image_path = data['image_path']\n",
    "        polygons = data['polygons']\n",
    "        pred_classes = data['pred_classes']\n",
    "        entropies = data['entropies']\n",
    "\n",
    "        if image_path is None:\n",
    "            continue\n",
    "\n",
    "        xtile, ytile = extract_xtile_ytile(image_path)\n",
    "        top_left, top_right, bottom_left, bottom_right = tile_corners_to_latlon(xtile, ytile, zoom)\n",
    "        latitude, longitude = calculate_tile_center(top_left, top_right, bottom_left, bottom_right)\n",
    "\n",
    "        for pred_class, polygon, entropy_value in zip(pred_classes, polygons, entropies):\n",
    "            if entropy_value is None or entropy_value > entropy_threshold:\n",
    "                print(f\"Skipped {pred_class} due to high entropy: {entropy_value if entropy_value is not None else 'None'}\")\n",
    "                continue\n",
    "\n",
    "            row = [image_path, pred_class, latitude, longitude, top_left[0], top_left[1],\n",
    "                   top_right[0], top_right[1], bottom_left[0], bottom_left[1], bottom_right[0], bottom_right[1]]\n",
    "\n",
    "            # Flatten polygon coordinates while ensuring it matches max_vertices\n",
    "            flat_polygon = [coord for point in polygon for coord in point]\n",
    "            flat_polygon += [None] * (2 * max_vertices - len(flat_polygon))  # Fill missing values with None\n",
    "\n",
    "            row.extend(flat_polygon)\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"CSV file '{csv_file}' saved successfully.\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hZptNR-tTe9R",
   "metadata": {
    "id": "hZptNR-tTe9R",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Add Buffer to combine nearby predicted objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23f70ee5-70ec-487f-8ba1-1a72010eb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "EARTH_CIRCUMFERENCE_DEGREES = 360  # degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68e76b28-709b-4ff4-91d4-46f897c48543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "df.rename(columns={'Predicted Class': 'Class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9356336-872a-4944-9b62-ec653b86eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the base name dynamically (e.g., \"TRY\" from \"TRY.csv\")\n",
    "csv_basename = os.path.splitext(os.path.basename(csv_file))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "-5H8fQokvr-W",
   "metadata": {
    "id": "-5H8fQokvr-W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_414326/2854683474.py:69: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf_final['Buffered'] = gdf_final.geometry.buffer(buffer_distance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ZIP file: Shapefile_Output/Masalia_Ponds_COMBINED_GEOMETRY.zip\n",
      "Deleted: Shapefile_Output/Masalia_Ponds_COMBINED_GEOMETRY.dbf\n",
      "Deleted: Shapefile_Output/Masalia_Ponds_COMBINED_GEOMETRY.shx\n",
      "Deleted: Shapefile_Output/Masalia_Ponds_COMBINED_GEOMETRY.shp\n",
      "Deleted: Shapefile_Output/Masalia_Ponds_COMBINED_GEOMETRY.cpg\n",
      "Deleted: Shapefile_Output/Masalia_Ponds_COMBINED_GEOMETRY.prj\n",
      "All shapefile components have been deleted after zipping, except for the .csv file.\n"
     ]
    }
   ],
   "source": [
    "# Function to convert pixel coordinates to geo-coordinates\n",
    "def pixel_to_geo(x, y, lat_top_left, lon_top_left, lat_bottom_right, lon_bottom_right, img_width, img_height):\n",
    "    lon_range = lon_bottom_right - lon_top_left\n",
    "    lat_range = lat_top_left - lat_bottom_right  # Latitude decreases southward\n",
    "    lon = lon_top_left + (x / img_width) * lon_range\n",
    "    lat = lat_top_left - (y / img_height) * lat_range\n",
    "    return lon, lat\n",
    "\n",
    "# Initialize an empty list for GeoJSON features\n",
    "geojson_features = []\n",
    "\n",
    "# Iterate through each row\n",
    "for _, row in df.iterrows():\n",
    "    lat_top_left = row['Top Left Latitude']\n",
    "    lon_top_left = row['Top Left Longitude']\n",
    "    lat_bottom_right = row['Bottom Right Latitude']\n",
    "    lon_bottom_right = row['Bottom Right Longitude']\n",
    "\n",
    "    tile_width, tile_height = 256, 256\n",
    "\n",
    "    object_coords = []\n",
    "\n",
    "    # Iterate over all possible coordinate columns dynamically\n",
    "    i = 1\n",
    "    while True:\n",
    "        x_col = f'X_{i}'\n",
    "        y_col = f'Y_{i}'\n",
    "\n",
    "        if x_col not in row or y_col not in row:\n",
    "            break  # Stop if columns don't exist\n",
    "\n",
    "        x = row[x_col]\n",
    "        y = row[y_col]\n",
    "\n",
    "        if pd.notna(x) and pd.notna(y):\n",
    "            lon, lat = pixel_to_geo(x, y, lat_top_left, lon_top_left, lat_bottom_right, lon_bottom_right, tile_width, tile_height)\n",
    "            if np.isfinite(lon) and np.isfinite(lat):\n",
    "                object_coords.append((lon, lat))\n",
    "        else:\n",
    "            break  # Stop when NaN values appear\n",
    "\n",
    "        i += 1  # Move to the next set of coordinates\n",
    "\n",
    "    # Ensure the polygon has at least 3 points before adding\n",
    "    if len(object_coords) >= 3:\n",
    "        object_coords.append(object_coords[0])  # Close the polygon\n",
    "        polygon_geometry = Polygon(object_coords)\n",
    "        feature = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": polygon_geometry,\n",
    "            \"properties\": {\n",
    "                \"Class\": row['Class']\n",
    "            }\n",
    "        }\n",
    "        geojson_features.append(feature)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf_final = gpd.GeoDataFrame(\n",
    "    [feature['properties'] for feature in geojson_features],\n",
    "    geometry=[feature['geometry'] for feature in geojson_features],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Merge overlapping geometries\n",
    "\n",
    "\n",
    "# Merge overlapping geometries\n",
    "buffer_distance = 0.0005  \n",
    "gdf_final['Buffered'] = gdf_final.geometry.buffer(buffer_distance)\n",
    "combined_polygons = unary_union(gdf_final['Buffered'])\n",
    "combined_polygons = combined_polygons.buffer(-buffer_distance)\n",
    "\n",
    "# Convert back to GeoDataFrame\n",
    "gdf_combined = gpd.GeoDataFrame(geometry=[combined_polygons], crs=\"EPSG:4326\")\n",
    "\n",
    "# Ensure the output folder exists\n",
    "output_folder = 'Shapefile_Output'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the final shapefile in the 'Shapefile_Output' folder\n",
    "shapefile_path = os.path.join(output_folder, f\"{csv_basename}_COMBINED_GEOMETRY.shp\")\n",
    "gdf_combined.to_file(shapefile_path)\n",
    "\n",
    "# Create the ZIP file in the 'Shapefile_Output' folder\n",
    "zip_filename = os.path.join(output_folder, f\"{csv_basename}_COMBINED_GEOMETRY.zip\")\n",
    "shapefile_files = glob.glob(os.path.join(output_folder, f\"{csv_basename}_COMBINED_GEOMETRY.*\"))  \n",
    "\n",
    "# Exclude the .csv file from the list of files to be zipped\n",
    "shapefile_files = [file for file in shapefile_files if not file.endswith('.csv')]\n",
    "\n",
    "# Add the shapefile files to the zip archive\n",
    "with zipfile.ZipFile(zip_filename, \"w\") as zipf:\n",
    "    for file in shapefile_files:\n",
    "        zipf.write(file, os.path.basename(file))\n",
    "\n",
    "print(f\"Created ZIP file: {zip_filename}\")\n",
    "\n",
    "# Delete the original shapefile files after zipping, but keep the .csv file\n",
    "for file in shapefile_files:\n",
    "    os.remove(file)\n",
    "    print(f\"Deleted: {file}\")\n",
    "\n",
    "print(\"All shapefile components have been deleted after zipping, except for the .csv file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83e361-e515-4ed3-b37f-80375464a620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "44b0a294",
    "ndTzVih5V4y3",
    "vvCWT5XYXaWn",
    "11a13c2e",
    "W8gnQ06EqxwU",
    "7vBqqRJWDOxu",
    "hZptNR-tTe9R"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
